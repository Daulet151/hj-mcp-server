"""
Analytical Agent
Analyzes data extraction queries using schema documentation from YML files
Executes SQL and provides real data insights
"""
import re
from typing import Dict, Any, Optional, Tuple
from anthropic import Anthropic
import pandas as pd
from utils.logger import setup_logger

logger = setup_logger(__name__, "INFO")


class AnalyticalAgent:
    """Analyzes queries and provides insights with real data before Excel export."""

    def __init__(
        self,
        api_key: str,
        schema_docs: Dict[str, Any],
        sql_generator,
        db_manager,
        model: str = "claude-sonnet-4-5-20250929"
    ):
        """
        Initialize analytical agent.

        Args:
            api_key: Anthropic API key
            schema_docs: Schema documentation loaded from YML files
            sql_generator: SQLGenerator instance for query generation
            db_manager: DatabaseManager instance for query execution
            model: Model to use for analysis
        """
        self.client = Anthropic(api_key=api_key)
        self.model = model
        self.schema_docs = schema_docs
        self.sql_generator = sql_generator
        self.db_manager = db_manager

        # Build context from schema documentation
        self.schema_context = self._build_schema_context()

        self.analysis_prompt = """–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö Hero's Journey. –¢–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –†–ï–ê–õ–¨–ù–´–ï –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ –¥–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π, –ø—Ä–∏ —ç—Ç–æ–º –æ—á–µ–Ω—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑.

**–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:**

1. –ù–∞—á–Ω–∏ —Å –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–ù–∞—à–µ–ª X –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
2. –î–∞–π 5-10 –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤ (bullets)
3. –ï—Å–ª–∏ –µ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –ø–æ–∫–∞–∂–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
4. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ, —É–ø–æ–º—è–Ω–∏ —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –ø–æ–ø—Ä–æ—Å–∏—Ç—å –≤—ã–≥—Ä—É–∑–∫—É –≤ Excel (—Å–∫–∞–∂–∏ "–≤—ã–≥—Ä—É–∑–∏ –≤ Excel" –µ—Å–ª–∏ –Ω—É–∂–µ–Ω —Ñ–∞–π–ª)

**–°—Ç–∏–ª—å:**
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, –Ω–µ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã
- –î—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Ç–æ–Ω
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ —É–º–µ—Ä–µ–Ω–Ω–æ
- –ù–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –ö—Ä–∞—Ç–∫–æ—Å—Ç—å –∏ —Ü–µ–Ω–Ω–æ—Å—Ç—å

**–ü—Ä–∏–º–µ—Ä —Ö–æ—Ä–æ—à–µ–≥–æ –æ—Ç–≤–µ—Ç–∞:**

–ù–∞—à–µ–ª 61 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —É –∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è HeroPass –Ω–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ (—Å 10 –ø–æ 17 –Ω–æ—è–±—Ä—è):

–û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã:
‚Ä¢ –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å–æ–∫ –∏—Å—Ç–µ–∫–∞—é—Ç 10-16 –Ω–æ—è–±—Ä—è
‚Ä¢ –ü—Ä–µ–æ–±–ª–∞–¥–∞—é—Ç –ì–æ–¥–æ–≤—ã–µ Hero's Pass (–≥–æ–¥–æ–≤—ã–µ –∞–±–æ–Ω–µ–º–µ–Ω—Ç—ã)
‚Ä¢ –ü–µ—Ä–≤—ã–µ –∏—Å—Ç–µ—á–µ–Ω–∏—è —É–∂–µ —Å–µ–≥–æ–¥–Ω—è –≤ 19:00 (10 –Ω–æ—è–±—Ä—è)

–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–∞—Ç–∞–º –æ–∫–æ–Ω—á–∞–Ω–∏—è:
‚Ä¢ 10 –Ω–æ—è–±—Ä—è - 12 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
‚Ä¢ 11 –Ω–æ—è–±—Ä—è - 8 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
‚Ä¢ 12 –Ω–æ—è–±—Ä—è - 5 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

–ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –≤—ã–≥—Ä—É–∑–∫–∞ ‚Äî —Å–∫–∞–∂–∏ "–≤—ã–≥—Ä—É–∑–∏ –≤ Excel" üìä"""

    def _build_schema_context(self) -> str:
        """Build schema context from YML documentation."""
        context_parts = []

        # Add tables information
        if "tables" in self.schema_docs:
            context_parts.append("**–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã:**\n")
            tables_dict = self.schema_docs["tables"]

            # Iterate through tables dictionary
            for table_name, table_info in tables_dict.items():
                description = table_info.get("description", "")
                context_parts.append(f"- `{table_name}`: {description}")

                # Add key columns
                columns = table_info.get("columns", [])
                if columns:
                    key_columns = [f"`{col['name']}` ({col.get('description', '')})"
                                   for col in columns[:5]]  # First 5 columns
                    context_parts.append(f"  –ö–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è: {', '.join(key_columns)}")

                context_parts.append("")

        # Add business terms from glossary
        if "glossary" in self.schema_docs:
            context_parts.append("\n**–ë–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω—ã (–≥–ª–æ—Å—Å–∞—Ä–∏–π):**\n")
            glossary = self.schema_docs["glossary"]
            for term, definition in list(glossary.items())[:10]:  # First 10 terms
                context_parts.append(f"- **{term}**: {definition}")

        return "\n".join(context_parts)

    def analyze(self, user_query: str, conversation_context: dict = None) -> Tuple[str, Optional[pd.DataFrame], Optional[str]]:
        """
        Analyze user's data extraction query by executing SQL and analyzing results.
        Retries up to 2 times if SQL execution fails, passing the error back to Claude.

        Args:
            user_query: User's data extraction request
            conversation_context: Optional dict with previous conversation context for follow-up queries

        Returns:
            Tuple of (analysis_text, dataframe, sql_query)
            - analysis_text: Analysis with insights and question
            - dataframe: Query results (for Excel generation later)
            - sql_query: Generated SQL query
        """
        max_retries = 10
        last_error = None
        last_sql = None
        tried_tables = set()  # Track schema.table already tried to avoid repeat loops

        for attempt in range(max_retries + 1):
            try:
                logger.info(f"Analyzing query with real data (attempt {attempt + 1}): {user_query[:100]}")

                # Step 1: Generate SQL query (pass error from previous attempt for self-correction)
                logger.info("Generating SQL query...")
                if attempt == 0:
                    sql_query = self.sql_generator.generate_query(user_query, conversation_context)
                else:
                    logger.info(f"Retrying SQL generation with error feedback: {last_error}")
                    sql_query = self.sql_generator.generate_query_with_error(
                        user_query, last_sql, str(last_error), conversation_context,
                        tried_tables=tried_tables
                    )
                last_sql = sql_query
                logger.info(f"Generated SQL: {sql_query[:100]}...")

                # Track which schema.table was used in this attempt
                for m in re.finditer(r'FROM\s+([\w]+)\.([\w]+)', sql_query, re.IGNORECASE):
                    tried_tables.add(f"{m.group(1)}.{m.group(2)}")
                if tried_tables:
                    logger.info("Tried tables so far: %s", ", ".join(sorted(tried_tables)))

                # Step 2: Execute query
                logger.info("Executing SQL query...")
                df = self.db_manager.execute_query(sql_query)

                # Step 3: Check if we have data ‚Äî treat empty result as failure and retry
                if df is None or df.empty:
                    logger.warning("Query returned no data, will retry with different approach")
                    last_error = "–ó–∞–ø—Ä–æ—Å –≤–µ—Ä–Ω—É–ª 0 —Å—Ç—Ä–æ–∫. –î–∞–Ω–Ω—ã—Ö –≤ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü–µ –Ω–µ—Ç ‚Äî –Ω—É–∂–Ω–∞ –¥—Ä—É–≥–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∏–ª–∏ —Å—Ö–µ–º–∞."
                    if attempt < max_retries:
                        continue
                    # All retries exhausted ‚Äî return empty result message
                    return (
                        "–ó–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ, –Ω–æ –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã—Ö –¥–∞–∂–µ –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫. –í–æ–∑–º–æ–∂–Ω–æ, —Ç–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –≤ –±–∞–∑–µ. ü§î",
                        None,
                        sql_query
                    )

                logger.info(f"Query returned {len(df)} rows √ó {len(df.columns)} columns")

                # Step 3.5: Enrich DataFrame ‚Äî resolve bare IDs into human-readable names
                df = self._enrich_id_columns(df)

                # Step 4: Prepare data summary for analysis
                data_summary = self._create_data_summary(df)

                # Step 5: Analyze with Claude
                logger.info("Analyzing data with AI...")
                response = self.client.messages.create(
                    model=self.model,
                    system=self.analysis_prompt,
                    messages=[
                        {"role": "user", "content": f"""–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_query}

–î–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã:
{data_summary}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∏ –¥–∞–π –∏–Ω—Å–∞–π—Ç—ã."""}
                    ],
                    temperature=0.7,
                    max_tokens=1000
                )

                analysis = response.content[0].text.strip()
                logger.info("Analysis with real data generated successfully")

                # Save successful pattern to bot_query_patterns for future reuse
                try:
                    self.db_manager.save_query_pattern(
                        question=user_query,
                        sql_query=sql_query,
                        row_count=len(df)
                    )
                except Exception as save_err:
                    logger.warning("Could not save query pattern: %s", save_err)

                return (analysis, df, sql_query)

            except Exception as e:
                last_error = e
                logger.error(f"Error during analysis (attempt {attempt + 1}): {e}")
                if attempt < max_retries:
                    logger.info(f"Will retry with error feedback...")
                    continue

        return (
            f"""–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö: {str(last_error)} üòî

–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ —É—Ç–æ—á–Ω–∏—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏.""",
            None,
            last_sql
        )

    # Maps a FK column name to (schema, table, name_column)
    # These are the most common FK columns that carry bare IDs users never want to see
    _FK_LOOKUPS = {
        "award":        ("raw",         "award",         "name"),
        "marathonevent":("raw",         "marathonevent", "name"),
        "event":        ("raw",         "event",         "name"),
        "clan":         ("raw",         "clan",          "name"),
        "user":         ("raw",         "user",          "username"),
        "booking":      ("raw",         "booking",       "id"),   # no good name col, skip
    }
    # MongoDB-style ObjectId: exactly 24 hex chars
    _OBJECTID_RE = re.compile(r'^[0-9a-f]{24}$', re.IGNORECASE)

    def _enrich_id_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Post-process DataFrame: for columns that contain bare IDs (FK references),
        lookup the human-readable name and add a new '*_name' column next to the ID column.
        Modifies df in place and returns it.
        """
        if df is None or df.empty:
            return df

        for col in list(df.columns):
            # Skip if already has a sibling _name column
            if f"{col}_name" in df.columns:
                continue

            col_lower = col.lower()

            # Check if column is a known FK
            lookup = self._FK_LOOKUPS.get(col_lower)

            # Also auto-detect: column name ends in 'id' or the values look like MongoDB ObjectIds
            if lookup is None:
                sample_vals = df[col].dropna().astype(str).head(5).tolist()
                all_objectids = sample_vals and all(self._OBJECTID_RE.match(v) for v in sample_vals)
                if not all_objectids:
                    continue
                # Try to guess table from column name (e.g. "awardid" ‚Üí "award")
                guessed_table = col_lower.rstrip("id").rstrip("_")
                lookup = ("raw", guessed_table, "name")

            schema, table, name_col = lookup
            # Skip degenerate lookups (e.g. booking has no useful name)
            if name_col == "id":
                continue

            # Collect unique IDs to look up
            unique_ids = df[col].dropna().astype(str).unique().tolist()
            if not unique_ids:
                continue

            try:
                placeholders = ",".join(["%s"] * len(unique_ids))
                sql = f'SELECT id, "{name_col}" FROM "{schema}"."{table}" WHERE id IN ({placeholders})'
                with self.db_manager.get_connection() as conn:
                    with conn.cursor() as cur:
                        cur.execute(sql, unique_ids)
                        rows = cur.fetchall()

                if not rows:
                    continue

                id_to_name = {row[0]: row[1] for row in rows}
                new_col = f"{col}_name"
                df.insert(
                    df.columns.get_loc(col) + 1,
                    new_col,
                    df[col].astype(str).map(id_to_name)
                )
                resolved = sum(1 for v in df[new_col] if v and str(v) != "nan")
                logger.info("Enriched column '%s' ‚Üí '%s': %d/%d IDs resolved from %s.%s",
                            col, new_col, resolved, len(df), schema, table)

            except Exception as e:
                logger.warning("Could not enrich column '%s' from %s.%s: %s", col, schema, table, e)

        return df

    def _create_data_summary(self, df: pd.DataFrame) -> str:
        """Create a concise summary of DataFrame for AI analysis."""
        summary_parts = []

        # Basic stats
        summary_parts.append(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
        summary_parts.append(f"–ö–æ–ª–æ–Ω–∫–∏: {', '.join(df.columns.tolist())}")

        # Show first few rows
        summary_parts.append("\n–ü–µ—Ä–≤—ã–µ –∑–∞–ø–∏—Å–∏:")
        summary_parts.append(df.head(10).to_string(index=False))

        # Value counts for categorical columns (if reasonable size)
        for col in df.columns:
            if df[col].dtype == 'object' and df[col].nunique() < 20:
                value_counts = df[col].value_counts()
                summary_parts.append(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ '{col}':")
                summary_parts.append(value_counts.head(10).to_string())

        # Date columns distribution
        date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower() or '_at' in col.lower()]
        for col in date_cols:
            try:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                if not df[col].isna().all():
                    summary_parts.append(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ '{col}':")
                    date_counts = df[col].dt.date.value_counts().sort_index()
                    summary_parts.append(date_counts.head(15).to_string())
            except:
                pass

        return "\n".join(summary_parts)
