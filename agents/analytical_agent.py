"""
Analytical Agent
Analyzes data extraction queries using schema documentation from YML files
Executes SQL and provides real data insights
"""
from typing import Dict, Any, Optional, Tuple
from openai import OpenAI
import pandas as pd
from utils.logger import setup_logger

logger = setup_logger(__name__, "INFO")


class AnalyticalAgent:
    """Analyzes queries and provides insights with real data before Excel export."""

    def __init__(
        self,
        api_key: str,
        schema_docs: Dict[str, Any],
        sql_generator,
        db_manager,
        model: str = "gpt-4o"
    ):
        """
        Initialize analytical agent.

        Args:
            api_key: OpenAI API key
            schema_docs: Schema documentation loaded from YML files
            sql_generator: SQLGenerator instance for query generation
            db_manager: DatabaseManager instance for query execution
            model: Model to use for analysis
        """
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.schema_docs = schema_docs
        self.sql_generator = sql_generator
        self.db_manager = db_manager

        # Build context from schema documentation
        self.schema_context = self._build_schema_context()

        self.analysis_prompt = """–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö Hero's Journey. –¢–µ–±–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –†–ï–ê–õ–¨–ù–´–ï –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ –¥–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑.

**–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:**

1. –ù–∞—á–Ω–∏ —Å –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–ù–∞—à–µ–ª X –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
2. –î–∞–π 3-5 –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤ (bullets)
3. –ï—Å–ª–∏ –µ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –ø–æ–∫–∞–∂–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
4. –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∑–∞–∫–æ–Ω—á–∏ –≤–æ–ø—Ä–æ—Å–æ–º: "–ñ–µ–ª–∞–µ—Ç–µ —á—Ç–æ–±—ã —è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –¥–ª—è –≤–∞—Å —Ç–∞–±–ª–∏—Ü—É —Å —ç—Ç–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏? üìä"

**–°—Ç–∏–ª—å:**
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, –Ω–µ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã
- –î—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Ç–æ–Ω
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ —É–º–µ—Ä–µ–Ω–Ω–æ
- –ù–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –ö—Ä–∞—Ç–∫–æ—Å—Ç—å –∏ —Ü–µ–Ω–Ω–æ—Å—Ç—å

**–ü—Ä–∏–º–µ—Ä —Ö–æ—Ä–æ—à–µ–≥–æ –æ—Ç–≤–µ—Ç–∞:**

–ù–∞—à–µ–ª 61 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —É –∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è HeroPass –Ω–∞ —ç—Ç–æ–π –Ω–µ–¥–µ–ª–µ (—Å 10 –ø–æ 17 –Ω–æ—è–±—Ä—è):

–û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã:
‚Ä¢ –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å–æ–∫ –∏—Å—Ç–µ–∫–∞—é—Ç 10-16 –Ω–æ—è–±—Ä—è
‚Ä¢ –ü—Ä–µ–æ–±–ª–∞–¥–∞—é—Ç –ì–æ–¥–æ–≤—ã–µ Hero's Pass (–≥–æ–¥–æ–≤—ã–µ –∞–±–æ–Ω–µ–º–µ–Ω—Ç—ã)
‚Ä¢ –ü–µ—Ä–≤—ã–µ –∏—Å—Ç–µ—á–µ–Ω–∏—è —É–∂–µ —Å–µ–≥–æ–¥–Ω—è –≤ 19:00 (10 –Ω–æ—è–±—Ä—è)

–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–∞—Ç–∞–º –æ–∫–æ–Ω—á–∞–Ω–∏—è:
‚Ä¢ 10 –Ω–æ—è–±—Ä—è - 12 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
‚Ä¢ 11 –Ω–æ—è–±—Ä—è - 8 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
‚Ä¢ 12 –Ω–æ—è–±—Ä—è - 5 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

–ñ–µ–ª–∞–µ—Ç–µ —á—Ç–æ–±—ã —è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –¥–ª—è –≤–∞—Å —Ç–∞–±–ª–∏—Ü—É —Å —ç—Ç–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏? üìä"""

    def _build_schema_context(self) -> str:
        """Build schema context from YML documentation."""
        context_parts = []

        # Add tables information
        if "tables" in self.schema_docs:
            context_parts.append("**–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã:**\n")
            tables_dict = self.schema_docs["tables"]

            # Iterate through tables dictionary
            for table_name, table_info in tables_dict.items():
                description = table_info.get("description", "")
                context_parts.append(f"- `{table_name}`: {description}")

                # Add key columns
                columns = table_info.get("columns", [])
                if columns:
                    key_columns = [f"`{col['name']}` ({col.get('description', '')})"
                                   for col in columns[:5]]  # First 5 columns
                    context_parts.append(f"  –ö–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è: {', '.join(key_columns)}")

                context_parts.append("")

        # Add business terms from glossary
        if "glossary" in self.schema_docs:
            context_parts.append("\n**–ë–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω—ã (–≥–ª–æ—Å—Å–∞—Ä–∏–π):**\n")
            glossary = self.schema_docs["glossary"]
            for term, definition in list(glossary.items())[:10]:  # First 10 terms
                context_parts.append(f"- **{term}**: {definition}")

        return "\n".join(context_parts)

    def analyze(self, user_query: str) -> Tuple[str, Optional[pd.DataFrame], Optional[str]]:
        """
        Analyze user's data extraction query by executing SQL and analyzing results.

        Args:
            user_query: User's data extraction request

        Returns:
            Tuple of (analysis_text, dataframe, sql_query)
            - analysis_text: Analysis with insights and question
            - dataframe: Query results (for Excel generation later)
            - sql_query: Generated SQL query
        """
        try:
            logger.info(f"Analyzing query with real data: {user_query[:100]}")

            # Step 1: Generate SQL query
            logger.info("Generating SQL query...")
            sql_query = self.sql_generator.generate_query(user_query)
            logger.info(f"Generated SQL: {sql_query[:100]}...")

            # Step 2: Execute query
            logger.info("Executing SQL query...")
            df = self.db_manager.execute_query(sql_query)

            # Step 3: Check if we have data
            if df is None or df.empty:
                logger.warning("Query returned no data")
                return (
                    "–ó–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ, –Ω–æ –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã—Ö. –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è–º. ü§î",
                    None,
                    sql_query
                )

            logger.info(f"Query returned {len(df)} rows √ó {len(df.columns)} columns")

            # Step 4: Prepare data summary for analysis
            data_summary = self._create_data_summary(df)

            # Step 5: Analyze with OpenAI
            logger.info("Analyzing data with AI...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.analysis_prompt},
                    {"role": "user", "content": f"""–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_query}

–î–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã:
{data_summary}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∏ –¥–∞–π –∏–Ω—Å–∞–π—Ç—ã."""}
                ],
                temperature=0.7,
                max_tokens=1000
            )

            analysis = response.choices[0].message.content.strip()
            logger.info("Analysis with real data generated successfully")

            # Ensure the question is present
            if "—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –¥–ª—è –≤–∞—Å —Ç–∞–±–ª–∏—Ü—É" not in analysis.lower() and \
               "—Å–≥–µ–Ω–µ—Ä–∏—Ä—É—é —Ç–∞–±–ª–∏—Ü—É" not in analysis.lower():
                analysis += "\n\n–ñ–µ–ª–∞–µ—Ç–µ —á—Ç–æ–±—ã —è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –¥–ª—è –≤–∞—Å —Ç–∞–±–ª–∏—Ü—É —Å —ç—Ç–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏? üìä"

            return (analysis, df, sql_query)

        except Exception as e:
            logger.error(f"Error during analysis: {e}")
            return (
                f"""–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)} üòî

–ù–æ —è –º–æ–≥—É –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –Ω–∞–ø—Ä—è–º—É—é.

–ñ–µ–ª–∞–µ—Ç–µ —á—Ç–æ–±—ã —è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –¥–ª—è –≤–∞—Å —Ç–∞–±–ª–∏—Ü—É —Å —ç—Ç–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏? üìä""",
                None,
                None
            )

    def _create_data_summary(self, df: pd.DataFrame) -> str:
        """Create a concise summary of DataFrame for AI analysis."""
        summary_parts = []

        # Basic stats
        summary_parts.append(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
        summary_parts.append(f"–ö–æ–ª–æ–Ω–∫–∏: {', '.join(df.columns.tolist())}")

        # Show first few rows
        summary_parts.append("\n–ü–µ—Ä–≤—ã–µ –∑–∞–ø–∏—Å–∏:")
        summary_parts.append(df.head(10).to_string(index=False))

        # Value counts for categorical columns (if reasonable size)
        for col in df.columns:
            if df[col].dtype == 'object' and df[col].nunique() < 20:
                value_counts = df[col].value_counts()
                summary_parts.append(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ '{col}':")
                summary_parts.append(value_counts.head(10).to_string())

        # Date columns distribution
        date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower() or '_at' in col.lower()]
        for col in date_cols:
            try:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                if not df[col].isna().all():
                    summary_parts.append(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ '{col}':")
                    date_counts = df[col].dt.date.value_counts().sort_index()
                    summary_parts.append(date_counts.head(15).to_string())
            except:
                pass

        return "\n".join(summary_parts)
